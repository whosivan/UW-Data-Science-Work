{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plan for Python work\n",
    "\n",
    "1) My _suggestion_ is to proceed as before and work through the notebook , switching every 5 or so minutes, discussing in each cell what the lines mean.  You may do something differently if you both agree. \n",
    "\n",
    "2) I provide a number of commands below (and prompts) , some of the things you need to figure out yourself and some of the tasks I have completed for you \n",
    "\n",
    "3) Make sure you understand how much time you have (ask me if you don't know) and plan accordingly. There is a lot of infromation in here!\n",
    "\n",
    "4) Plenty of suggestions at the bottom for more things to try - you should take a look and make sure you can do all of these things... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#A bunch of libraries and packages \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import urllib2\n",
    "\n",
    "from sklearn import linear_model\n",
    "from pandas.tools.plotting import scatter_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# UCI ML database - energy efficiency\n",
    "# Database of many ML data available here: https://archive.ics.uci.edu/ml/\n",
    "\n",
    "#what the heck is this doing? \n",
    "socket = urllib2.urlopen('https://archive.ics.uci.edu/ml/machine-learning-databases/00242/ENB2012_data.xlsx')\n",
    "UCI_energy=pd.read_excel(socket)\n",
    "\n",
    "# definition of dataframes \n",
    "# X1\tRelative Compactness \n",
    "# X2\tSurface Area \n",
    "# X3\tWall Area \n",
    "# X4\tRoof Area \n",
    "# X5\tOverall Height \n",
    "# X6\tOrientation \n",
    "# X7\tGlazing Area \n",
    "# X8\tGlazing Area Distribution \n",
    "# y1\tHeating Load \n",
    "# y2\tCooling Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# This is a naive validation set approach. Please understand and briefly discuss this is just for teaching\n",
    "# What would you do in the real world based on our bootstrap/resampling lessons? \n",
    "train,test=train_test_split(UCI_energy,test_size=0.05,random_state=1010)\n",
    "\n",
    "# have you had a look at the data yet? quickly do so before moving on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Multiple linear regression on X1-X8 predicting Y1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train linear model \n",
    "MLR=linear_model.LinearRegression()\n",
    "MLR.fit(train[['X1','X2','X3','X4','X5','X6','X7','X8']],train.Y1)\n",
    "\n",
    "# make predictions on test and train set \n",
    "trainpred=MLR.predict(train[['X1','X2','X3','X4','X5','X6','X7','X8']])\n",
    "testpred=MLR.predict(test[['X1','X2','X3','X4','X5','X6','X7','X8']])\n",
    "\n",
    "#make parity plot \n",
    "plt.figure(figsize=(4,4))\n",
    "plt.xlim([0,50]);\n",
    "plt.ylim([0,50]);\n",
    "plt.scatter(train.Y1,trainpred)\n",
    "plt.scatter(test.Y1,testpred,color='r')\n",
    "plt.plot([0,50],[0,50],lw=4,color='black')\n",
    "\n",
    "#calculate the test and train error\n",
    "print(\"Train error\",mean_squared_error(train.Y1,trainpred))\n",
    "print(\"Test error\",mean_squared_error(test.Y1,testpred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Ridge Regression (same data as Part 1)\n",
    "\n",
    "* The Ridge coefficients minimize $RSS + \\lambda \\sum_{j=1}^{p}\\beta_j^2$\n",
    "    * There is an additional **penalty** in error for having nonzero coefficients!\n",
    "* Note: Eq 6.5 in ISLR shows the tuning parameter as $\\lambda$, it is $\\alpha$ in SKLearn\n",
    "* Goal here: train models as a function of the regularization parameter \n",
    "* The X's should be normalized as in Eq 6.6, there is a normalization feature, but we will do it manually using $x_{ij}=\\frac{x_{ij}}{s_j}$\n",
    "    * I suggest on your own you test out what normalization in Ridge does\n",
    "    * Some methods in sklearn also do automatic selection of shrinkage coefficient! Cool! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#normalized data for Ridge / LASSO \n",
    "train_normalized=train/train.std()\n",
    "test_normalized=test/test.std()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-1 Example of single instance of RR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "heat_ridge=Ridge()\n",
    "a=1e0\n",
    "heat_ridge.set_params(alpha=a)\n",
    "heat_ridge.fit(train_normalized[['X1','X2','X3','X4','X5','X6','X7','X8']],train_normalized.Y1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0818431981701\n",
      "1.24921520518\n"
     ]
    }
   ],
   "source": [
    "print mean_squared_error(train_normalized.Y1,heat_ridge.predict(\n",
    "        train_normalized[['X1','X2','X3','X4','X5','X6','X7','X8']]))\n",
    "\n",
    "print mean_squared_error(test_normalized.Y1,heat_ridge.predict(\n",
    "        test_normalized[['X1','X2','X3','X4','X5','X6','X7','X8']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2-2 Example of searching the $\\alpha$ space in RR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# RR vs lambda (based on sklearn tutorial)\n",
    "coefs = []\n",
    "trainerror = []\n",
    "testerror = []\n",
    "\n",
    "# do you know what is happening here? \n",
    "lambdas = np.logspace(-6,6,200)\n",
    "model=Ridge()\n",
    "\n",
    "# loop over lambda values (strength of regularization)\n",
    "for l in lambdas:\n",
    "    model.set_params(alpha=l)\n",
    "    model.fit(train_normalized[['X1','X2','X3','X4','X5','X6','X7','X8']],train_normalized.Y1)\n",
    "    coefs.append(model.coef_)\n",
    "    trainerror.append(mean_squared_error(train_normalized.Y1,model.predict(\n",
    "        train_normalized[['X1','X2','X3','X4','X5','X6','X7','X8']])))\n",
    "    testerror.append(mean_squared_error(test_normalized.Y1,model.predict(\n",
    "        test_normalized[['X1','X2','X3','X4','X5','X6','X7','X8']])))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what is being plotted here? \n",
    "plt.figure(figsize=(10,3))\n",
    "plt.subplot(121)\n",
    "plt.plot(lambdas,coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('coefs')\n",
    "plt.title('RR coefs vs $\\lambda$')\n",
    "plt.subplot(122)\n",
    "plt.plot(lambdas,trainerror,label='train error')\n",
    "plt.plot(lambdas,testerror,label='test error')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('error')\n",
    "plt.legend(loc=1)\n",
    "plt.title('error vs $\\lambda$')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RR questions \n",
    "\n",
    "1) Explain to each other what is happening in these two plots   \n",
    "2) Why does the blue curve have a minimum at the smallest $\\lambda4 value? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: LASSO regression  (same data as Part 1)\n",
    "\n",
    "* The lasso improves over ridge by also providing a variable selection tool!\n",
    "* The lasso minimizer is $RSS + \\lambda \\sum_{j=1}^{p}\\lvert\\beta_j\\rvert$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# also based on sklearn tutorials\n",
    "# what the hell is happening in this cell?\n",
    "coefs = []\n",
    "trainerror = []\n",
    "testerror = []\n",
    "\n",
    "lambdas = np.logspace(-6,6,200)\n",
    "model=linear_model.Lasso()\n",
    "\n",
    "# loop over lambda values (strength of regularization)\n",
    "for l in lambdas:\n",
    "    model.set_params(alpha=l,max_iter=1e6)\n",
    "    model.fit(train_normalized[['X1','X2','X3','X4','X5','X6','X7','X8']],train_normalized.Y1)\n",
    "    coefs.append(model.coef_)\n",
    "    trainerror.append(mean_squared_error(train_normalized.Y1,model.predict(\n",
    "        train_normalized[['X1','X2','X3','X4','X5','X6','X7','X8']])))\n",
    "    testerror.append(mean_squared_error(test_normalized.Y1,model.predict(\n",
    "        test_normalized[['X1','X2','X3','X4','X5','X6','X7','X8']])))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,3))\n",
    "#plt.locator_params(nbins=5)\n",
    "plt.subplot(121)\n",
    "plt.plot(lambdas,coefs)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('coefs')\n",
    "plt.title('RR coefs vs $\\lambda$')\n",
    "#plt.xlim(1e-4,1e0)\n",
    "\n",
    "plt.subplot(122)\n",
    "plt.plot(lambdas,trainerror,label='train error')\n",
    "plt.plot(lambdas,testerror,label='test error')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('$\\lambda$')\n",
    "plt.ylabel('error')\n",
    "#plt.xlim(1e-4,1e0)\n",
    "#plt.ylim(0,0.5)\n",
    "plt.legend(loc=1)\n",
    "plt.title('error vs $\\lambda$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Other things to consider if you have more time \n",
    "\n",
    "* Note we did not scale the features in the MLR, try it out and verify the final error doesnt' change!\n",
    "* Make sure you undersand how to make _predictions_ with supervised learning models that are trained on scaled/normalized data\n",
    "* Plot the residuals and verify if errors are distributed normally\n",
    "* Make a parity plot including the predictions from ridge and LASSO \n",
    "* Compare errors between all three \n",
    "* Explore the effect of training/testing split \n",
    "* Look at the shrinkage/regularization situation when predicting Y2 vs Y1... \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
